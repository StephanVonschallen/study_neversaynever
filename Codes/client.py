# src/client.py

from __future__ import annotations

import os
from dataclasses import dataclass, field
from typing import Any, Dict, List, Literal, Optional, Protocol, Tuple

from dotenv import load_dotenv

from .config import MODEL_CONFIG  # expected to be a dict[str, dict[str, Any]]

# --- Optional imports by provider ----------------------------------------------------

try:  # OpenAI
    from openai import OpenAI  # type: ignore
except ImportError:  # pragma: no cover
    OpenAI = None  # type: ignore

try:  # Anthropic (Claude)
    import anthropic  # type: ignore
except ImportError:  # pragma: no cover
    anthropic = None  # type: ignore

try:  # Gemini (google-genai)
    from google import genai  # type: ignore
except ImportError:  # pragma: no cover
    genai = None  # type: ignore

try:  # Mistral API
    from mistralai import Mistral  # type: ignore
except ImportError:  # pragma: no cover
    Mistral = None  # type: ignore

try:  # HuggingFace local (LLaMA / Mistral / etc.)
    import torch  # type: ignore
    from transformers import AutoModelForCausalLM, AutoTokenizer  # type: ignore
except ImportError:  # pragma: no cover
    torch = None  # type: ignore
    AutoModelForCausalLM = None  # type: ignore
    AutoTokenizer = None  # type: ignore


Role = Literal["system", "user", "assistant"]


# --- Data structures -----------------------------------------------------------------


@dataclass
class TokenLogProb:
    """Token-level log-probability information."""

    token: str
    logprob: float
    position: int  # token index in the generated message


@dataclass
class MessageStats:
    """
    Representation of a single message in the conversation.

    Note: for user/system messages, tokens and token_logprobs will usually be None.
    """

    role: Role
    content: str
    tokens: Optional[List[str]] = None
    token_logprobs: Optional[List[TokenLogProb]] = None


@dataclass
class ConversationResult:
    """
    Result of a full conversation run for a given model.

    - messages: full conversation (system, user, assistant, in order)
    - assistant_messages: subset of messages that were generated by the model
    - raw_responses: provider-specific raw responses (one per assistant message)
    """

    model_name: str
    messages: List[MessageStats]
    assistant_messages: List[MessageStats]
    raw_responses: List[Any] = field(default_factory=list)


class LLMClient(Protocol):
    """
    Generic interface for an LLM client.

    A concrete implementation (OpenAI, Anthropic, etc.) must implement run_conversation.
    """

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,
        **gen_kwargs: Any,
    ) -> ConversationResult:
        ...


# --- Shared helpers ------------------------------------------------------------------


def _get_model_config(model_name: str) -> Dict[str, Any]:
    """
    Helper to fetch model configuration from MODEL_CONFIG.

    MODEL_CONFIG can contain multiple entries per provider, e.g.:

        "gpt-3.5-turbo", "gpt-4.1", "gpt-5.1-thinking" for OpenAI
        "gemini-1.0-pro", "gemini-1.5-pro" for Gemini, etc.

    Each entry must specify:
        - provider
        - model_id (API identifier)
        - env_var (optional)
        - supports_logprobs (bool)  # per-model flag
    """
    try:
        return MODEL_CONFIG[model_name]
    except KeyError as exc:
        raise ValueError(f"Unknown model '{model_name}'. Check your config.py.") from exc


def _resolve_api_key(cfg: Dict[str, Any], explicit_api_key: Optional[str]) -> str:
    """
    Resolve API key either from an explicit argument or from environment variables.

    Uses cfg.get("env_var") if present, otherwise falls back to common provider-specific names.
    """
    if explicit_api_key:
        return explicit_api_key

    load_dotenv()

    provider = cfg.get("provider")
    env_var = cfg.get("env_var")

    # Providers that don't require API keys (e.g. local HuggingFace)
    if provider in ("huggingface", "local", "llama_local"):
        return ""

    # Prefer per-model env var if configured
    if env_var:
        key = os.getenv(env_var)
        if key:
            return key
        raise RuntimeError(
            f"API key for model is expected in environment variable '{env_var}', "
            "but it is not set."
        )

    # Fallback heuristics by provider
    if provider == "openai":
        key = os.getenv("OPENAI_API_KEY")
        if key:
            return key
    elif provider == "anthropic":
        key = os.getenv("ANTHROPIC_API_KEY")
        if key:
            return key
    elif provider == "gemini":
        key = os.getenv("GEMINI_API_KEY")
        if key:
            return key
    elif provider == "mistral":
        key = os.getenv("MISTRAL_API_KEY")
        if key:
            return key

    raise RuntimeError(
        "API key not provided and could not be resolved from environment. "
        "Pass api_key explicitly or set an env var as configured in MODEL_CONFIG."
    )


def _normalize_logprob_flags(
    user_prompts: List[str], request_logprobs: Optional[List[bool]]
) -> List[bool]:
    """
    Normalize log-probability flags to a list[bool] of length len(user_prompts).

    - None  -> all False
    - [True/False] with length 1 -> broadcast to all prompts
    - [True/False] with len == len(user_prompts) -> used as-is
    """
    n = len(user_prompts)
    if request_logprobs is None:
        return [False] * n

    if len(request_logprobs) == 1 and n > 1:
        return request_logprobs * n

    if len(request_logprobs) != n:
        raise ValueError(
            "request_logprobs must be either None, length 1, "
            f"or the same length as user_prompts (got {len(request_logprobs)})."
        )

    return request_logprobs


# --- OpenAI client -------------------------------------------------------------------


class OpenAIChatClient:
    """
    Concrete LLM client for OpenAI chat models.

    Supports multiple model variants via MODEL_CONFIG:
        - gpt-3.5-turbo, gpt-4.1, gpt-5.1-thinking, etc.

    Per-model logprob support is controlled by `supports_logprobs` in config.py.
    """

    def __init__(self, api_key: str):
        if OpenAI is None:
            raise ImportError(
                "openai package is not installed. Add 'openai' to requirements.txt."
            )
        self._client = OpenAI(api_key=api_key)

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,
        **gen_kwargs: Any,
    ) -> ConversationResult:
        if not user_prompts:
            raise ValueError("user_prompts must contain at least one prompt string.")

        request_logprobs = _normalize_logprob_flags(user_prompts, request_logprobs)

        cfg = _get_model_config(model_name)
        model_id = cfg.get("model_id", model_name)
        supports_logprobs = bool(cfg.get("supports_logprobs", False))

        messages: List[Dict[str, str]] = []
        result_messages: List[MessageStats] = []
        assistant_messages: List[MessageStats] = []
        raw_responses: List[Any] = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
            result_messages.append(MessageStats(role="system", content=system_prompt))

        for idx, user_prompt in enumerate(user_prompts):
            messages.append({"role": "user", "content": user_prompt})
            result_messages.append(MessageStats(role="user", content=user_prompt))

            want_logprobs = request_logprobs[idx] and supports_logprobs

            response = self._client.chat.completions.create(
                model=model_id,
                messages=messages,
                logprobs=want_logprobs or None,  # type: ignore[arg-type]
                top_logprobs=1 if want_logprobs else None,
                **gen_kwargs,
            )
            raw_responses.append(response)

            assistant_content, token_stats = self._extract_message_and_logprobs(response)
            messages.append({"role": "assistant", "content": assistant_content})

            msg_stats = MessageStats(
                role="assistant",
                content=assistant_content,
                tokens=[t.token for t in token_stats] if token_stats else None,
                token_logprobs=token_stats if token_stats else None,
            )
            result_messages.append(msg_stats)
            assistant_messages.append(msg_stats)

        return ConversationResult(
            model_name=model_name,
            messages=result_messages,
            assistant_messages=assistant_messages,
            raw_responses=raw_responses,
        )

    @staticmethod
    def _extract_message_and_logprobs(
        response: Any,
    ) -> Tuple[str, Optional[List[TokenLogProb]]]:
        choice = response.choices[0]

        content = getattr(choice.message, "content", "")
        if isinstance(content, list):
            text_parts = []
            for part in content:
                if isinstance(part, dict) and "text" in part:
                    text_parts.append(part["text"])
                elif hasattr(part, "text"):
                    text_parts.append(part.text)  # type: ignore[attr-defined]
            content = "".join(text_parts)
        elif content is None:
            content = ""

        token_stats: Optional[List[TokenLogProb]] = None
        logprobs = getattr(choice, "logprobs", None)

        if logprobs and hasattr(logprobs, "content"):
            token_stats = []
            for idx, item in enumerate(logprobs.content):
                token = getattr(item, "token", None)
                lp = getattr(item, "logprob", None)
                if token is None or lp is None:
                    continue
                token_stats.append(TokenLogProb(token=token, logprob=float(lp), position=idx))

        return str(content), token_stats


# --- Anthropic (Claude) client -------------------------------------------------------


class AnthropicChatClient:
    """
    LLM client for Anthropic Claude models via anthropic.messages API.

    Claude does NOT expose token-level logprobs in current public API, so
    token_logprobs=None in MessageStats.
    """

    def __init__(self, api_key: str):
        if anthropic is None:
            raise ImportError(
                "anthropic package is not installed. Add 'anthropic' to requirements.txt."
            )
        self._client = anthropic.Anthropic(api_key=api_key)

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,  # ignored for Claude
        **gen_kwargs: Any,
    ) -> ConversationResult:

        if not user_prompts:
            raise ValueError("user_prompts must contain at least one prompt string.")

        cfg = _get_model_config(model_name)
        model_id = cfg.get("model_id", model_name)

        # Claude requires `max_tokens`, so ensure it exists.
        max_tokens = gen_kwargs.pop("max_tokens", 512)

        # ---- Build Claude-style messages list ----
        # Claude requires everything inside `messages=[ ... ]`
        # No separate system=... parameter is allowed.
        claude_messages: List[Dict[str, str]] = []
        result_messages: List[MessageStats] = []
        assistant_messages: List[MessageStats] = []
        raw_responses: List[Any] = []

        # Add system message inside messages list (correct Anthropic format)
        if system_prompt:
            claude_messages.append({
                "role": "system",
                "content": system_prompt
            })
            result_messages.append(
                MessageStats(role="system", content=system_prompt, token_logprobs=None)
            )

        # ---- Multi-turn conversation ----
        for user_prompt in user_prompts:

            # User message
            claude_messages.append({
                "role": "user",
                "content": user_prompt
            })
            result_messages.append(
                MessageStats(role="user", content=user_prompt, token_logprobs=None)
            )

            # Call Claude API
            response = self._client.messages.create(
                model=model_id,
                messages=claude_messages,
                max_tokens=max_tokens,
                **gen_kwargs
            )
            raw_responses.append(response)

            # Extract assistant text
            text_parts: List[str] = []
            for block in response.content:
                # Claude response chunks have typed blocks, e.g. {"type": "text", "text": "..."}
                if getattr(block, "type", None) == "text":
                    text_parts.append(block.text)

            assistant_text = "".join(text_parts)

            # Append assistant message to conversation history
            claude_messages.append({
                "role": "assistant",
                "content": assistant_text
            })

            # Build MessageStats for assistant reply
            msg_stats = MessageStats(
                role="assistant",
                content=assistant_text,
                token_logprobs=None,   # Claude does not expose per-token logprobs
            )
            result_messages.append(msg_stats)
            assistant_messages.append(msg_stats)

        return ConversationResult(
            model_name=model_name,
            messages=result_messages,
            assistant_messages=assistant_messages,
            raw_responses=raw_responses,
        )


class GeminiChatClient:
    """
    Simple Gemini chat wrapper for gemini-1.0-pro and future 1.5 models.

    - For 1.0 models: no system_instruction support → inject system prompt into contents.
    - For 1.5 models (if you add them): system_instruction is set when constructing the model.
    """

    def __init__(self, api_key: str):
        try:
            import google.generativeai as genai
        except ImportError:
            raise ImportError(
                "Please install google-generativeai: pip install google-generativeai"
            )

        genai.configure(api_key=api_key)
        self.genai = genai

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,
        **gen_kwargs,
    ) -> ConversationResult:
        from .config import MODEL_CONFIG  # avoid circular import at import time

        model_cfg = MODEL_CONFIG[model_name]
        model_id = model_cfg["model_id"]
        supports_system = bool(model_cfg.get("supports_system_instruction", False))

        # ------------------------------------------------------------------ #
        # Build contents for Gemini
        # ------------------------------------------------------------------ #
        contents = []

        # For 1.0 models: encode system as a pseudo-system message
        if system_prompt and not supports_system:
            contents.append({
                "role": "user",
                "parts": [{"text": f"[SYSTEM]\n{system_prompt}"}],
            })

        # Add each user prompt as a turn
        for up in user_prompts:
            contents.append({
                "role": "user",
                "parts": [{"text": up}],
            })

        # Generation config
        gen_config = {
            "max_output_tokens": gen_kwargs.get("max_tokens", 512),
            "temperature": gen_kwargs.get("temperature", 0.7),
        }

        # ------------------------------------------------------------------ #
        # Create the GenerativeModel and call generate_content
        # ------------------------------------------------------------------ #
        try:
            if not supports_system:
                # Gemini 1.0: no system_instruction arg
                model = self.genai.GenerativeModel(model_id)
            else:
                # Future Gemini 1.5: system_instruction is passed here
                model = self.genai.GenerativeModel(
                    model_id,
                    system_instruction=system_prompt,
                )

            response = model.generate_content(
                contents,
                generation_config=gen_config,
            )

        except Exception as e:
            print(f"[GeminiChatClient] Error calling model {model_id}: {e}")
            raise

        # ------------------------------------------------------------------ #
        # Extract text
        # ------------------------------------------------------------------ #
        try:
            text_output = response.text
        except Exception:
            # Fallback: try candidates
            try:
                text_output = response.candidates[0].content.parts[0].text
            except Exception:
                text_output = ""

        tokens = text_output.split()

        msg_stats = MessageStats(
            content=text_output,
            role="assistant",
            token_logprobs=None,
        )


        return ConversationResult(
            assistant_messages=[msg_stats],
            model_name=model_name,
        )


# --- Mistral API client --------------------------------------------------------------


class MistralChatClient:
    """
    LLM client for Mistral models via mistralai.Mistral.

    Token-level logprobs are currently not requested; tokens/logprobs will be None.
    """

    def __init__(self, api_key: str):
        if Mistral is None:
            raise ImportError(
                "mistralai package is not installed. Add 'mistralai' to requirements.txt."
            )
        self._client = Mistral(api_key=api_key)

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,  # ignored for now
        **gen_kwargs: Any,
    ) -> ConversationResult:
        if not user_prompts:
            raise ValueError("user_prompts must contain at least one prompt string.")

        cfg = _get_model_config(model_name)
        model_id = cfg.get("model_id", model_name)

        messages: List[Dict[str, str]] = []
        result_messages: List[MessageStats] = []
        assistant_messages: List[MessageStats] = []
        raw_responses: List[Any] = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
            result_messages.append(MessageStats(role="system", content=system_prompt))

        for user_prompt in user_prompts:
            messages.append({"role": "user", "content": user_prompt})
            result_messages.append(MessageStats(role="user", content=user_prompt))

            response = self._client.chat.complete(
                model=model_id,
                messages=messages,
                **gen_kwargs,
            )
            raw_responses.append(response)

            choice = response.choices[0]
            assistant_content = getattr(choice.message, "content", "") or ""

            messages.append({"role": "assistant", "content": assistant_content})

            msg_stats = MessageStats(
                role="assistant",
                content=assistant_content,
                tokens=None,
                token_logprobs=None,
            )
            result_messages.append(msg_stats)
            assistant_messages.append(msg_stats)

        return ConversationResult(
            model_name=model_name,
            messages=result_messages,
            assistant_messages=assistant_messages,
            raw_responses=raw_responses,
        )


# --- HuggingFace local client (LLaMA / Mistral etc.) ---------------------------------


class HuggingFaceLocalClient:
    """
    LLM client for local HuggingFace models (e.g. LLaMA, Mistral) using transformers.

    This implementation:
    - builds a text prompt from (optional) system + conversation transcript,
    - generates new tokens with `model.generate`,
    - uses `output_scores=True` to compute token-level log-probs for the generated tokens.
    """

    def __init__(self, model_id: str, device: Optional[str] = None):
        if AutoModelForCausalLM is None or AutoTokenizer is None or torch is None:
            raise ImportError(
                "transformers and torch are required for HuggingFaceLocalClient. "
                "Install them (e.g. 'pip install transformers torch')."
            )

        self.model_id = model_id
        self.device = device or (
            "cuda" if torch.cuda.is_available() else "cpu"  # type: ignore[attr-defined]
        )

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(model_id)
        self.model.to(self.device)
        self.model.eval()

    def run_conversation(
        self,
        model_name: str,
        user_prompts: List[str],
        system_prompt: Optional[str] = None,
        request_logprobs: Optional[List[bool]] = None,
        **gen_kwargs: Any,
    ) -> ConversationResult:
        if not user_prompts:
            raise ValueError("user_prompts must contain at least one prompt string.")

        request_logprobs = _normalize_logprob_flags(user_prompts, request_logprobs)

        result_messages: List[MessageStats] = []
        assistant_messages: List[MessageStats] = []

        transcript = ""

        if system_prompt:
            transcript += f"[SYSTEM]: {system_prompt}\n"
            result_messages.append(MessageStats(role="system", content=system_prompt))

        raw_responses: List[Any] = []

        for idx, user_prompt in enumerate(user_prompts):
            transcript += f"[USER]: {user_prompt}\n"
            result_messages.append(MessageStats(role="user", content=user_prompt))

            full_prompt = transcript + "[ASSISTANT]:"

            inputs = self.tokenizer(
                full_prompt, return_tensors="pt", add_special_tokens=False
            ).to(self.device)

            max_new_tokens = gen_kwargs.pop("max_new_tokens", 256)

            with torch.no_grad():  # type: ignore[attr-defined]
                output = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    return_dict_in_generate=True,
                    output_scores=True,
                    **gen_kwargs,
                )

            seq = output.sequences[0]
            input_len = inputs["input_ids"].shape[1]
            generated_ids = seq[input_len:]

            scores = output.scores  # list[tensor[batch, vocab]]
            tokens = self.tokenizer.convert_ids_to_tokens(generated_ids)

            token_stats: List[TokenLogProb] = []
            if request_logprobs[idx]:
                for pos, (tok_id, logits) in enumerate(zip(generated_ids, scores)):
                    log_probs = torch.nn.functional.log_softmax(logits[0], dim=-1)  # type: ignore[attr-defined]
                    lp = float(log_probs[tok_id])
                    token_stats.append(
                        TokenLogProb(
                            token=tokens[pos],
                            logprob=lp,
                            position=pos,
                        )
                    )

            assistant_text = self.tokenizer.decode(
                generated_ids, skip_special_tokens=True
            )
            transcript += f" {assistant_text}\n"

            msg_stats = MessageStats(
                role="assistant",
                content=assistant_text,
                tokens=tokens if token_stats else None,
                token_logprobs=token_stats if token_stats else None,
            )
            result_messages.append(msg_stats)
            assistant_messages.append(msg_stats)

            raw_responses.append(
                {
                    "generated_ids": generated_ids.tolist(),
                    "tokens": tokens,
                    "token_logprobs": [t.logprob for t in token_stats]
                    if token_stats
                    else None,
                }
            )

        return ConversationResult(
            model_name=model_name,
            messages=result_messages,
            assistant_messages=assistant_messages,
            raw_responses=raw_responses,
        )


# --- Factory / public API ------------------------------------------------------------


def create_llm_client(model_name: str, api_key: Optional[str] = None) -> LLMClient:
    """
    Factory that returns an LLM client appropriate for the configured provider.

    Multiple models per provider are supported via MODEL_CONFIG, e.g.:

        - "gpt-3.5-turbo", "gpt-4.1", "gpt-5.1-thinking" (OpenAI)
        - "gemini-1.0-pro", "gemini-1.5-pro" (Gemini)
        - "claude-3-opus", "claude-3.5-sonnet" (Anthropic)
    """
    cfg = _get_model_config(model_name)
    provider = cfg.get("provider")

    if provider in ("openai", "anthropic", "gemini", "mistral"):
        key = _resolve_api_key(cfg, api_key)
    else:
        key = api_key or ""

    if provider == "openai":
        return OpenAIChatClient(api_key=key)
    if provider == "anthropic":
        return AnthropicChatClient(api_key=key)
    if provider == "gemini":
        return GeminiChatClient(api_key=key)
    if provider == "mistral":
        return MistralChatClient(api_key=key)
    if provider in ("huggingface", "local", "llama_local"):
        model_id = cfg.get("model_id", model_name)
        return HuggingFaceLocalClient(model_id=model_id)

    raise NotImplementedError(
        f"Provider '{provider}' is not implemented yet in client.py. "
        "Add a concrete client for this provider."
    )


def run_conversation(
    model_name: str,
    user_prompts: List[str],
    api_key: Optional[str] = None,
    system_prompt: Optional[str] = None,
    request_logprobs: Optional[List[bool]] = None,
    **gen_kwargs: Any,
) -> ConversationResult:
    """
    Convenience function so you don’t need to interact with the client class directly.

    Inputs:
        - model_name: key in MODEL_CONFIG (e.g. "gpt-4.1", "claude-3.5-sonnet", "gemini-1.5-pro")
        - user_prompts: list of user messages (one assistant reply per prompt)
        - api_key: explicit API key (otherwise resolved via .env/env vars)
        - system_prompt: optional system message prepended to the conversation
        - request_logprobs: optional list of booleans, one per user prompt, or length 1.
          True means: request or compute token log-probs for that assistant reply
          (where the model/client supports it).
        - gen_kwargs: extra generation kwargs (temperature, max_tokens, etc.)

    Output:
        ConversationResult with:
        - .messages: full conversation (system + user + assistant)
        - .assistant_messages: only the generated messages
        - token-level log-probs where available (None otherwise)
    """
    client = create_llm_client(model_name, api_key=api_key)
    return client.run_conversation(
        model_name=model_name,
        user_prompts=user_prompts,
        system_prompt=system_prompt,
        request_logprobs=request_logprobs,
        **gen_kwargs,
    )


__all__ = [
    "TokenLogProb",
    "MessageStats",
    "ConversationResult",
    "LLMClient",
    "create_llm_client",
    "run_conversation",
]
